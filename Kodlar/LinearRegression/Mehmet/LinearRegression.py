import streamlit as st
import numpy as np
import plotly.graph_objects as go
import matplotlib.pyplot as plt
import time
import pandas as pd

st.set_page_config(layout="wide")
st.title("Linear Regression")


st.markdown(
    """Linear regresyonda amaÃ§ girdi olarak kabul ettiÄŸimiz baÄŸÄ±mlÄ± deÄŸiÅŸken olan x deÄŸerlerine karÅŸÄ±lÄ±k gelen hedef deÄŸiÅŸkenimiz y ile doÄŸrusal bir fonksiyon bulmak.
                Genellikle Ã§ok bÃ¼yÃ¼k boyutlu olmayan sayÄ±sal verilerdeki doÄŸrusal iliÅŸki yakalamak veya ileriye dÃ¶nÃ¼k tahminlerde bulunmak iÃ§in kullanÄ±lmaktadÄ±r.
                Genelde kullanÄ±lan formullerin aÅŸÅŸaÄŸÄ±da gÃ¶rÃ¼dÃ¼ÄŸÃ¼nÃ¼z aynÄ± anlama gelen farklÄ± alanlara ve basitliÄŸe sahip gÃ¶sterimler var.
                """
)
st.latex(r"y = f(x) + \epsilon")
# Basit gÃ¶sterim
st.latex(
    r"""
    y = \beta_0 + \beta_1 x
    """
)

# Spesifik gÃ¶sterim
st.latex(
    r"""
    y = \beta_0 + \beta_1 x + \epsilon
    """
)

st.markdown(
    """
                Epsilon deÄŸeri gerÃ§ek dÃ¼nya etkisindeki bilinmeyen ve tam olarak modellenemeyen faktÃ¶rdÃ¼r. Ã‡eÅŸitli veri hatalarÄ±nÄ± iÃ§erir ve independenty and identically distrubed (i.d.d) olarak bilinir yani baÄŸÄ±msÄ±zdÄ±rlar  ve aynÄ± olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±na aittirler.
              """
)
st.markdown("""Î²0 : Y-intercept (kesim noktasÄ±), modelin baÅŸlangÄ±Ã§ deÄŸeri. """)
st.markdown(
    """Î²1 : EÄŸim katsayÄ±sÄ±, baÄŸÄ±msÄ±z deÄŸiÅŸkenin baÄŸÄ±mlÄ± deÄŸiÅŸken Ã¼zerindeki etkisi olarak Ã¶zetlenebilir """
)
st.markdown("""Peki bu deÄŸerler nasÄ±l bulunur? """)

st.latex(
    r"\hat{\beta}_1 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}"
)

st.latex(r"\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}")


# Ã–rnek veri seti oluÅŸturma
np.random.seed(42)
x = 2 * np.random.rand(100)
y = 4 + 3 * x + np.random.randn(100)

# Ortalama deÄŸerler
x_mean = np.mean(x)
y_mean = np.mean(y)

# EÄŸim (beta_1) hesaplama
numerator = np.sum((x - x_mean) * (y - y_mean))
denominator = np.sum((x - x_mean) ** 2)
beta_1 = numerator / denominator

# KesiÅŸim noktasÄ± (beta_0) hesaplama
beta_0 = y_mean - beta_1 * x_mean


# Streamlit baÅŸlÄ±ÄŸÄ±
st.title("Lineer Regresyon UygulamasÄ±")

# Kod dizinini gÃ¶ster
code = """
    import numpy as np

    # Ã–rnek veri seti oluÅŸturma
    np.random.seed(42) # Rastegele sayÄ±larÄ±n kullanÄ±m tekrarÄ±nda aynÄ± sayÄ±lar olmasÄ± iÃ§in sabitliyoruz.
    x = 2 * np.random.rand(100) # 0-100 arasÄ±nda sayÄ±larÄ± 2 ile Ã§arparak geniÅŸletiyoruz.
    y = 4 + 3 * x + np.random.randn(100) # dogrusal fonkisoyuna tabii olan bir y deÄŸerleri olusturuyoruz.

    # Ortalama deÄŸerler
    x_mean = np.mean(x)
    y_mean = np.mean(y)

    # EÄŸim (beta_1) hesaplama 
    OrtalamaFark = np.sum((x - x_mean) * (y - y_mean)) # Ortalama farklarÄ±n carpÄ±mÄ± aralarÄ±ndaki kovaryansÄ± temsil etmektedir.
    xinVaryansÄ± = np.sum((x - x_mean) ** 2) # Burada da x'in farklarÄ±nÄ±n karesi ile x'in varyansÄ±nÄ± temsil etmiÅŸ oluyoruz.
    beta_1 = OrtalamaFark / xinVaryansÄ±    # Ä°ki deÄŸerin bÃ¶lÃ¼mÃ¼ bize eÄŸimi vermektedir.

    # KesiÅŸim noktasÄ± (beta_0) hesaplama
    beta_0 = y_mean - beta_1 * x_mean # Son olarak linear denklemden Î²1x'i cÄ±kararak x'in sÄ±fÄ±rdaki y deÄŸerni bulmuÅŸ oluyoruz. 
    
    
     # MSE hesaplama
    y_pred = beta_0 + beta_1 * x  #mse hesaplamak icin y thaminlerini buluyuoruz

    mse = np.mean((y - y_pred) ** 2) #GerÃ§ek deÄŸerler ile tahmin edilen deÄŸerler arasÄ±ndaki farklarÄ±n kareleri hesaplaplandÄ±ktan sonra ortalamasÄ±nÄ± kullanÄ±yoruz.
    """
st.code(code, language="python")

# HesaplamalarÄ± gÃ¶ster
st.write(f"Ortalama X = {x_mean:.3f}")
st.write(f"Ortalama Y = {y_mean:.3f}")
st.write(f"EÄŸim (beta_1)= {beta_1:.3f}")
st.write(f"KesiÅŸim noktasÄ± (beta_0) ={beta_0:.3f}")

y_pred = beta_0 + beta_1 * x
st.subheader("MSE Denklemi")
st.latex(r" MSE = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2")

# MSE hesaplama
mse = np.mean((y - y_pred) ** 2)
st.write(f"Mean Squared Error (MSE): {mse:.3f}")


# Grafik oluÅŸturma
fig = go.Figure()

# Veri noktalarÄ±nÄ± ekleme
fig.add_trace(
    go.Scatter(
        x=x, y=y, mode="markers", name="Veri NoktalarÄ±", marker=dict(color="cyan")
    )
)

# Regresyon doÄŸrusunu ekleme
fig.add_trace(
    go.Scatter(
        x=x,
        y=beta_0 + beta_1 * x,
        mode="lines",
        name="Regresyon DoÄŸrusu",
        line=dict(color="red"),
    )
)

# GrafiÄŸi stilize etme
fig.update_layout(
    plot_bgcolor="black",
    paper_bgcolor="black",
    font_color="white",
    title="Basit Lineer Regresyon",
    xaxis_title="X DeÄŸeri",
    yaxis_title="Y DeÄŸeri",
)

# GrafiÄŸi gÃ¶sterme
st.plotly_chart(fig)


# Rastgele veri oluÅŸtur
np.random.seed(42)
x = 2 * np.random.rand(100)
y = 4 + 3 * x + np.random.randn(100)

st.title("Uygulamadaki beta deÄŸerlerini deÄŸiÅŸtirerek sonuÃ§larÄ±nÄ± gÃ¶zlemleyelim.")

# KaydÄ±rma Ã§ubuklarÄ±
b0 = st.slider(
    "ğ›½0 (baÅŸlangÄ±Ã§ deÄŸeri)", min_value=-10.0, max_value=10.0, value=0.0, step=0.01
)
b1 = st.slider("ğ›½1 (eÄŸim)", min_value=-10.0, max_value=10.0, value=1.0, step=0.01)

# Regresyon modelini hesapla
y_pred = b0 + b1 * x

# Grafik oluÅŸtur
scatter_trace = go.Scatter(
    x=x, y=y, mode="markers", name="Veri NoktalarÄ±", marker=dict(color="white")
)
line_trace = go.Scatter(
    x=x, y=y_pred, mode="lines", name="Regresyon DoÄŸrusu", line=dict(color="red")
)

# FarklarÄ± gÃ¶steren dikey Ã§izgiler
difference_traces = [
    go.Scatter(
        x=[x[i], x[i]],
        y=[y[i], y_pred[i]],
        mode="lines",
        line=dict(color="gray", dash="dash", width=0.5),
        showlegend=False,
    )
    for i in range(len(x))
]

# Layout ayarlarÄ±
layout = go.Layout(
    title="Lineer Regresyon",
    xaxis=dict(title="X", color="white"),
    yaxis=dict(title="Y", color="white"),
    plot_bgcolor="black",
    paper_bgcolor="black",
    font=dict(color="white"),
)

# Grafik Ã§izimi
fig = go.Figure(data=[scatter_trace, line_trace] + difference_traces, layout=layout)

# GrafiÄŸi Streamlit ile gÃ¶ster
st.plotly_chart(fig)


st.header("Gradyan descent(iniÅŸi) ile parametre arayÄ±ÅŸÄ±")


# FormÃ¼llerin GÃ¶sterimi
st.latex(
    r"""
    J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2
    """
)

st.latex(
    r"""
    h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_n x_n
    """
)

st.latex(
    r"""
    \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)
    """
)

st.latex(
    r"""
    \theta_j := \theta_j - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}
    """
)

st.title("Gradyan Ä°niÅŸi ve DoÄŸrusal Regresyon FormÃ¼lleri")

# Maliyet Fonksiyonu
st.subheader("Maliyet Fonksiyonu (Cost Function)")
st.latex(
    r"""
        J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2
        """
)
st.write(
    """
        **AÃ§Ä±klama:**
        Maliyet fonksiyonu, tahminlerin gerÃ§ek deÄŸerlere ne kadar yakÄ±n olduÄŸunu Ã¶lÃ§er. 
        Bu formÃ¼l, tÃ¼m Ã¶rnekler Ã¼zerindeki ortalama kare hata (mean squared error) hesaplamaktadÄ±r. 
           """
)

# Hipotez Fonksiyonu
st.subheader("Hipotez Fonksiyonu (Hypothesis Function)")
st.latex(
    r"""
        h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_n x_n
        """
)
st.write(
    """
        **AÃ§Ä±klama:**
        Hipotez fonksiyonu, doÄŸrusal regresyon modelinin tahmin fonksiyonudur. 
        Modelin giriÅŸ Ã¶zellikleri ile parametrelerin Ã§arpÄ±mÄ±nÄ±n toplamÄ±nÄ± verir.
        """
)

# Gradyan Ä°niÅŸi GÃ¼ncelleme KuralÄ±
st.subheader("Gradyan Ä°niÅŸi GÃ¼ncelleme KuralÄ± (Gradient Descent Update Rule)")

st.latex(
    r"""
        \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)
        """
)
st.write(
    """
        **AÃ§Ä±klama:**
        Bu formÃ¼l, her parametrenin gradyanÄ±nÄ± kullanarak gÃ¼ncellenmesini saÄŸlar. 
        Burada, alpha Ã¶ÄŸrenme oranÄ± ve Ã§arpan durumunda olan maliyet fonksiyonunun Î¸j parametresine gÃ¶re tÃ¼revidir.
        """
)

# AÄŸÄ±rlÄ±k GÃ¼ncelleme KuralÄ±
st.subheader("AÄŸÄ±rlÄ±k GÃ¼ncelleme KuralÄ± (Weight Update Rule)")

st.latex(
    r"\theta_0 := \theta_0 - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})"
)
st.latex(
    r"""
        \theta_j := \theta_j - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}
        """
)
st.write(
    """
        **AÃ§Ä±klama:**
        Bu formÃ¼l, doÄŸrusal regresyon iÃ§in gradyan iniÅŸi algoritmasÄ±nÄ±n aÄŸÄ±rlÄ±k gÃ¼ncelleme kuralÄ±dÄ±r. 
        Her bir aÄŸÄ±rlÄ±k, gradyan (tÃ¼rev) bilgisi kullanÄ±larak gÃ¼ncellenir. 
        Burada parametrelere gÃ¶re maliyet fonksiyonunun gradyanÄ±nÄ± hesaplar.
        """
)

st.write(
    """
        **Ä°ÅŸlem SÄ±rasÄ±:**
        1. **Hipotez Fonksiyonu:** Modelin tahminlerini oluÅŸturur.
        2. **Maliyet Fonksiyonu:** Tahminlerin doÄŸruluÄŸunu deÄŸerlendirir.
        3. **Gradyan Ä°niÅŸi GÃ¼ncelleme KuralÄ±:** Parametrelerin gradyanÄ±nÄ± hesaplar.
        4. **AÄŸÄ±rlÄ±k GÃ¼ncelleme KuralÄ±:** Her bir aÄŸÄ±rlÄ±ÄŸÄ± gradyan bilgisi ile gÃ¼nceller.
        """
)


st.title("DoÄŸrusal Regresyon ve Gradyan Ä°niÅŸi Ã–rneÄŸi")

# Kod dizinini gÃ¶ster
code = """
    aynÄ± veri setini oluÅŸturuyoruz.
    np.random.seed(42)
    x = 2 * np.random.rand(100)
    y = 4 + 3 * x + np.random.randn(100)

    # Ã–ÄŸrenme oranÄ± ve iterasyon yanÄ± tekrar sayÄ±larÄ±nÄ± belirliyoruz
    alpha = st.slider('Ã–ÄŸrenme OranÄ± (Î±)', 0.001, 0.1, 0.01, 0.001)
    iterations = st.slider('Ä°terasyon SayÄ±sÄ±', 100, 5000, 1000, 100)

    # Hipotez fonksiyonunu yani y_pred (tahmini deÄŸerler)
    def hypothesis(theta0, theta1, x):
        return theta0 + theta1 * x

    # Maliyet fonksiyonunu 1/2 ile Ã¶zellÅŸtirilmiÅŸ bir mse fonksiyonudur amaÃ§ alfa ve iterasyon uygulamasÄ±nÄ±n kolaylaÅŸmasÄ±
    def compute_cost(theta0, theta1, x, y):
        return (1 / (2 * len(x))) * np.sum((hypothesis(theta0, theta1, x) - y) ** 2)

    # Gradyan iniÅŸi ile Î¸ deÄŸerlerinin gÃ¼ncellenmesi 
    def gradient_descent(x, y, alpha, iterations): # Ã¶ÄŸrenme oranÄ± ve tekrar sayÄ±sÄ± belirtilir.
        theta0 = 0
        theta1 = 0
        costs = []
    
        for _ in range(iterations): #tekrar sayÄ±sÄ± boyunca gradyan hesaplanÄ±r
            # GradyanÄ± hesapla
            gradients0 = (1 / len(x)) * np.sum(hypothesis(theta0, theta1, x) - y)
            gradients1 = (1 / len(x)) * np.sum((hypothesis(theta0, theta1, x) - y) * x)

            # Parametreleri gÃ¼ncelle # her tekrarda parametre gÃ¼ncellenir
            theta0 -= alpha * gradients0
            theta1 -= alpha * gradients1

            # Maliyet fonksiyonun gÃ¼ncellenmesi
            costs.append(compute_cost(theta0, theta1, x, y))

        return theta0, theta1, costs

    # Gradyan iniÅŸi iÅŸlemi
    theta0, theta1, costs = gradient_descent(x, y, alpha, iterations) 
    """
st.code(code, language="python")


# Veri setini oluÅŸtur
np.random.seed(42)
x = 2 * np.random.rand(100)
y = 4 + 3 * x + np.random.randn(100)

# Ã–ÄŸrenme oranÄ± ve iterasyon sayÄ±sÄ±
alpha = st.slider("Ã–ÄŸrenme OranÄ± (Î±)", 0.001, 0.1, 0.01, 0.001, format="%.3f")
iterations = st.slider("Ä°terasyon SayÄ±sÄ±", 100, 12000, 1000, 100)


# Hipotez fonksiyonunu tanÄ±mla
def hypothesis(theta0, theta1, x):
    return theta0 + theta1 * x


# Maliyet fonksiyonunu tanÄ±mla
def compute_cost(theta0, theta1, x, y):
    return (1 / (2 * len(x))) * np.sum((hypothesis(theta0, theta1, x) - y) ** 2)


# Gradyan iniÅŸi ile parametreleri gÃ¼ncelle
def gradient_descent(x, y, alpha, iterations):
    theta0 = 0
    theta1 = 0
    costs = []

    for _ in range(iterations):
        # GradyanÄ± hesapla
        gradients0 = (1 / len(x)) * np.sum(hypothesis(theta0, theta1, x) - y)
        gradients1 = (1 / len(x)) * np.sum((hypothesis(theta0, theta1, x) - y) * x)

        # Parametreleri gÃ¼ncelle
        theta0 -= alpha * gradients0
        theta1 -= alpha * gradients1

        # Maliyet fonksiyonunu kaydet
        costs.append(compute_cost(theta0, theta1, x, y))

    return theta0, theta1, costs


# Gradyan iniÅŸi iÅŸlemi
theta0, theta1, costs = gradient_descent(x, y, alpha, iterations)

# Veri seti ve tahminleri gÃ¶rselleÅŸtir
st.subheader("Veri Seti ve Hipotez Fonksiyonu")
fig, ax = plt.subplots()
fig.patch.set_facecolor("black")
ax.set_facecolor("black")
ax.spines["bottom"].set_color("white")
ax.spines["top"].set_color("white")
ax.spines["right"].set_color("white")
ax.spines["left"].set_color("white")
ax.tick_params(axis="x", colors="white")
ax.tick_params(axis="y", colors="white")
ax.scatter(x, y, color="blue", label="Veri")
x_vals = np.linspace(0, 2, 100)
y_vals = hypothesis(theta0, theta1, x_vals)
ax.plot(x_vals, y_vals, color="red", label="Tahmin")
ax.set_xlabel("x", color="white")
ax.set_ylabel("y", color="white")
ax.legend()
st.pyplot(fig)

# Maliyet fonksiyonunun evrimi
st.subheader("Maliyet Fonksiyonunun Evrimi")
fig, ax = plt.subplots()
fig.patch.set_facecolor("black")
ax.set_facecolor("black")
ax.spines["bottom"].set_color("white")
ax.spines["top"].set_color("white")
ax.spines["right"].set_color("white")
ax.spines["left"].set_color("white")
ax.tick_params(axis="x", colors="white")
ax.tick_params(axis="y", colors="white")
ax.plot(range(iterations), costs, color="green")
ax.set_xlabel("Ä°terasyon SayÄ±sÄ±", color="white")
ax.set_ylabel("Maliyet", color="white")
ax.set_ylim(0, 2)
st.pyplot(fig)

# SonuÃ§larÄ± yazdÄ±r
st.subheader("SonuÃ§lar")
st.write(f"SonuÃ§: Î¸0 = {theta0:.3f}, Î¸1 = {theta1:.3f}")
st.write(f"Son Maliyet: {costs[-1]:.3f}")

y_ = theta0 + theta1 * x

# MSE hesaplama
mseG = np.mean((y - y_) ** 2)
st.write(f"Mean Squared Error (MSE): {mseG:.3f}")


st.subheader("Linear Regresyon Similasyonu")

np.random.seed(42)
x = 2 * np.random.rand(100)
y = 4 + 3 * x + np.random.randn(100)


# Gradyan iniÅŸi fonksiyonu
def gradient_descent(x, y, learning_rate=0.001, epochs=500):
    m = x.size
    b0 = 0.0
    b1 = 0.0
    history = []

    for epoch in range(epochs):
        y_pred = b0 + b1 * x
        error = y - y_pred
        b0 -= learning_rate * (-2 * error.sum()) / m
        b1 -= learning_rate * (-2 * (x * error).sum()) / m
        if epoch % 10 == 0:  # Her 10 adÄ±mda bir kaydet
            history.append((b0, b1, y_pred.copy()))  # y_pred'i kopyalayarak kaydet

    return b0, b1, history


# Ã–ÄŸrenme oranÄ± ve epoch sayÄ±sÄ± iÃ§in kaydÄ±rma Ã§ubuklarÄ±
learning_rate = st.slider(
    "Ã–ÄŸrenme OranÄ±",
    min_value=0.001,
    max_value=0.05,
    value=0.001,
    step=0.001,
    key="learning_rate",
)
epochs = st.slider(
    "Epoch SayÄ±sÄ±", min_value=100, max_value=8000, value=100, step=100, key="epochs"
)

# Buton ekle ve tÄ±klama durumunu kontrol et
if st.button("Hesapla"):
    b0, b1, history = gradient_descent(x, y, learning_rate, epochs)

    # Ä°lk grafiÄŸi oluÅŸtur
    scatter_trace = go.Scatter(
        x=x, y=y, mode="markers", name="Veri NoktalarÄ±", marker=dict(color="white")
    )
    layout = go.Layout(
        title="Lineer Regresyon - AdÄ±m AdÄ±m",
        xaxis=dict(title="X", color="white"),
        yaxis=dict(title="Y", color="white"),
        plot_bgcolor="black",
        paper_bgcolor="black",
        font=dict(color="white"),
    )
    fig = go.Figure(data=[scatter_trace], layout=layout)
    plot = st.plotly_chart(fig, use_container_width=True)

    # SimÃ¼lasyon grafiÄŸi
    for b0, b1, y_pred in history:
        line_trace = go.Scatter(
            x=x,
            y=y_pred,
            mode="lines",
            name="Regresyon DoÄŸrusu",
            line=dict(color="red"),
        )

        # FarklarÄ± gÃ¶steren dikey Ã§izgiler
        difference_traces = [
            go.Scatter(
                x=[x[i], x[i]],
                y=[y[i], y_pred[i]],
                mode="lines",
                line=dict(color="gray", dash="dash", width=0.5),
                showlegend=False,
            )
            for i in range(len(x))
        ]

        # Verileri gÃ¼ncelle
        fig = go.Figure(
            data=[scatter_trace, line_trace] + difference_traces, layout=layout
        )
        plot.plotly_chart(fig, use_container_width=True)

        # AdÄ±mlar arasÄ±nda kÄ±sa bir duraklama ekleyin
        time.sleep(0.1)

    # Nihai parametre deÄŸerlerini gÃ¶ster
    st.write(f"ğ›½0: {b0:.3f}")
    st.write(f"ğ›½1: {b1:.3f}")
    y_p = b0 + b1 * x
    mseP = np.mean((y - y_p) ** 2)
    st.write(f"Mean Squared Error (MSE): {mseP:.3f}")


df = pd.DataFrame({"X": x, "Y": y})

x_range = np.linspace(x.min(), x.max(), 100)


df["ManuelLinear"] = beta_0 + beta_1 * df["X"]
df["GradyanLinear"] = theta0 + theta1 * df["X"]
df_first_10 = df.head(10)


st.title("Veri KÃ¼mesinin Ä°lk 10 Ã–rneÄŸi")

st.write("Ä°lk 10 Ã–rnek:")
st.dataframe(df_first_10)
